{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with multiple input variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Vertorization to implement multiple linear regression\n",
    "\n",
    "#### Multiple features (variables)\n",
    "\n",
    "| Size in feet^2 ($x_1$) | # of bedrooms ($x_2$)| # of floors ($x_3$) | Age of home ($x_4$) | Price ($y$) |\n",
    "|----------------|--------------|-------------|-------------|-------|\n",
    "| 2104           | 5            | 1           | 45          | 460   |\n",
    "| 1416           | 3            | 2           | 40          | 232   |\n",
    "| 852            | 2            | 1           | 35          | 178   |\n",
    "| ...            | ...          | ...         | ...         | ...   |\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "* $x^{(i)}$ is the input (features) of the $i^{th}$ training example.\n",
    "    $x^{(1)}$ = [2104, 5, 1, 45]\n",
    "* $ n $ is the number of features, number of columns in the matrix\n",
    "    $ n = 4 $\n",
    "* $ m $ is the number of training examples, number of rows in the matrix\n",
    "* $x_j$ is the input (features) of the $j^{th}$ feature.\n",
    "    $x_1$ = [2104, 1416, 852, ...]\n",
    "* $x^{(i)}_j$ is the input (features) of the $j^{th}$ feature of the $i^{th}$ training example.\n",
    "    $x^{(1)}_1$ = 2104\n",
    "\n",
    "The formula for model regresion is: $f_w,_b(x) = w*x + b$\n",
    "\n",
    "for the multiple features the formula is: \n",
    "\n",
    "\n",
    "\n",
    "$$f_w,_b(x) = w_1*x_1 + w_2*x_2 + ... + w_n*x_n + 1b$$\n",
    "$$ f_{\\vec{W},b}(\\vec{X}) = \\vec{W}\\cdot\\vec{X} + b $$\n",
    "\n",
    "#### Vectorization\n",
    "\n",
    "\n",
    "$$ \\vec{W} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix} \\quad $$\n",
    "$$ \\vec{X} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\quad $$\n",
    "$$ \\vec{W}\\cdot\\vec{X} = w_1*x_1 + w_2*x_2 + ... + w_n*x_n $$\n",
    "\n",
    "\n",
    "#### Without vectorization\n",
    "\n",
    "$$ f_{\\vec{W},b}(\\vec{X}) = \\vec{W}\\cdot\\vec{X} + b = \\sum_{j=1}^{n} w_j*x_j + b $$\n",
    "\n",
    "#### Gradient Descent\n",
    "\n",
    "$$ repeat \\{ $$\n",
    "$$ \\vec{W} = \\vec{W} - \\alpha * \\dfrac{\\partial}{\\partial \\vec{W}} J(\\vec{W},b) $$\n",
    "$$ b = b - \\alpha * \\dfrac{\\partial}{\\partial b} J(\\vec{W},b) $$\n",
    "$$ \\} $$\n",
    "$$ \\dfrac{\\partial}{\\partial \\vec{W}} J(\\vec{W},b) = \\dfrac{1}{m} \\sum_{i=1}^{m} (\\vec{W}\\cdot\\vec{X}^{(i)} + b - y^{(i)})\\vec{X}^{(i)} $$\n",
    "$$ \\dfrac{\\partial}{\\partial b} J(\\vec{W},b) = \\dfrac{1}{m} \\sum_{i=1}^{m} (\\vec{W}\\cdot\\vec{X}^{(i)} + b - y^{(i)}) $$\n",
    "\n",
    "An alternative to gradient descent is the **Normal Equation**:\n",
    "\n",
    "* No need to choose learning rate, Solve for $w$, $b$ with out iteration\n",
    "* Only for linear regression\n",
    "* Doesn't generalize to other models like logistic regression\n",
    "* Slow if $n$ is very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2104    5    1   45]\n",
      " [1416    3    2   40]\n",
      " [ 852    2    1   35]]\n",
      "[460 232 178]\n"
     ]
    }
   ],
   "source": [
    "# data same of the previous example in the table\n",
    "\n",
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785.1811367994083\n",
      "[  0.39133535  18.75376741 -53.36032453 -26.42131618]\n"
     ]
    }
   ],
   "source": [
    "# random initialization of the parameters\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(b_init)\n",
    "print(w_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_without_vectorization(X, w, b):\n",
    "    \"\"\"implementation of the prediction\n",
    "    Args:\n",
    "        X: input matrix\n",
    "        W: weights vector\n",
    "        b: bias\n",
    "    returns:\n",
    "        vector of predictions\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m = X.shape[0]\n",
    "    # number of features\n",
    "    n = X.shape[1]\n",
    "    # vector of predictions\n",
    "    y_hat = np.zeros(m)\n",
    "    \n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            y_hat[i] += X[i][j] * w[j]\n",
    "        y_hat[i] += b\n",
    "    return y_hat\n",
    "    \n",
    "\n",
    "def predict(X, w, b):\n",
    "    \"\"\"Vectorized implementation of the prediction\n",
    "    Args:\n",
    "        X: input matrix\n",
    "        W: weights vector\n",
    "        b: bias\n",
    "        \n",
    "    Returns: \n",
    "        vector of predictions\n",
    "    \"\"\"\n",
    "    y_hat = np.dot(X, w) + b\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: [459.99999762 231.99999837 177.99999899]\n",
      "prediction without vectorization: [459.99999762 231.99999837 177.99999899]\n"
     ]
    }
   ],
   "source": [
    "# prediction with the random parameters\n",
    "assert np.allclose(predict(X_train, w_init, b_init), predict_without_vectorization(X_train, w_init, b_init))   \n",
    "\n",
    "print(f'prediction: {predict(X_train, w_init, b_init)}')\n",
    "print(f'prediction without vectorization: {predict_without_vectorization(X_train, w_init, b_init)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cost function with multiple variables\n",
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"Compute the cost function\n",
    "    Args:\n",
    "        X: input matrix\n",
    "        y: target vector\n",
    "        w: weights vector\n",
    "        b: bias\n",
    "        \n",
    "    Returns:\n",
    "        cost function\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    y_hat = predict(X, w, b)\n",
    "    cost = (1/(2*m)) * np.sum((y_hat - y)**2)\n",
    "    return cost\n",
    "\n",
    "def compute_cost_without_vectorization(X, y, w, b):\n",
    "    \"\"\"Compute the cost function without vectorization\n",
    "    Args:\n",
    "        X: input matrix\n",
    "        y: target vector\n",
    "        w: weights vector\n",
    "        b: bias\n",
    "        \n",
    "    Returns:\n",
    "        cost function\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    y_hat = predict_without_vectorization(X, w, b)\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        cost = cost + (y_hat[i] - y[i])**2\n",
    "    # divide by 2m\n",
    "    cost = cost / (2*m)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute gradient multiple variables\n",
    "\n",
    "def compute_gradient(X, y, w, b, alpha):\n",
    "    \"\"\"Compute the gradient of the cost function\n",
    "    \n",
    "        args:\n",
    "            X: input matrix\n",
    "            y: target vector\n",
    "            w: weights vector\n",
    "            b: bias\n",
    "        returns:\n",
    "            w: weights vector gradient\n",
    "            b: bias gradient\n",
    "            cost: cost function\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = compute_cost(X, y, w, b)\n",
    "    dw = (1/m) * (predict(X, w, b) - y) @ X\n",
    "    db = (1/m) * sum(predict(X, w, b) - y)\n",
    "    #print('dw: ', dw)\n",
    "    #print('db: ', db)\n",
    "    w_temp = w - alpha * dw\n",
    "    b_temp = b - alpha * db\n",
    "    return w_temp, b_temp, cost\n",
    "    \n",
    "def compute_gradient_without_vectorization(X, y, w, b, alpha):\n",
    "    \"\"\"Compute the gradient of the cost function\n",
    "    \n",
    "        args:\n",
    "            X: input matrix\n",
    "            y: target vector\n",
    "            w: weights vector\n",
    "            b: bias\n",
    "        returns:\n",
    "            w: weights vector gradient\n",
    "            b: bias gradient\n",
    "            cost: cost function\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    dw = np.zeros(n)\n",
    "    db = 0\n",
    "    y_hat = predict_without_vectorization(X, w, b)\n",
    "    # cal partial derivatives\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            dw[j] += (y_hat[i] - y[i]) * X[i][j]\n",
    "        db += (y_hat[i] - y[i])\n",
    "    \n",
    "    # update parameters and divide by m\n",
    "    b_temp = b - (alpha * db) / m\n",
    "    w_temp = np.zeros(n)\n",
    "    for j in range(n):\n",
    "        w_temp[j] = w[j] - (alpha * dw[j]) / m\n",
    "        \n",
    "    return w_temp, b_temp, compute_cost_without_vectorization(X, y, w, b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1e-06\n",
      "Gradient descent multiple variables, vectorized implementation\n",
      "-----------------------------------\n",
      "(array([  0.39133535,  18.75376741, -53.36032453, -26.42131618]), 785.18113679941, 1.5578904045996674e-12)\n",
      "Gradient descent multiple variables, without vectorization\n",
      "-----------------------------------\n",
      "(array([  0.39133535,  18.75376741, -53.36032453, -26.42131618]), 785.18113679941, 1.5578904428966628e-12)\n"
     ]
    }
   ],
   "source": [
    "# gradient descent multiple variables\n",
    "alpha = 0.000001\n",
    "print(f'alpha: {alpha}')\n",
    "print('Gradient descent multiple variables, vectorized implementation')\n",
    "print('-----------------------------------')\n",
    "print(compute_gradient(X_train, y_train, w_init, b_init, alpha))\n",
    "print('Gradient descent multiple variables, without vectorization')\n",
    "print('-----------------------------------')\n",
    "print(compute_gradient_without_vectorization(X_train, y_train, w_init, b_init, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_iter(X, y, w, b, alpha, iterations, gd_fn):\n",
    "    \"\"\"Compute the gradient of the cost function\n",
    "    \n",
    "        args:\n",
    "            X: input matrix\n",
    "            y: target vector\n",
    "            w: weights vector\n",
    "            b: bias\n",
    "            alpha: learning rate\n",
    "            iterations: number of iterations\n",
    "            cost_fn: cost function\n",
    "        returns:\n",
    "            w: weights vector gradient\n",
    "            b: bias gradient\n",
    "            cost: cost function\n",
    "    \"\"\"\n",
    "    # cal time of execution\n",
    "    start = time.time()\n",
    "    costs = []\n",
    "    for i in range(iterations):\n",
    "        w, b, cost = gd_fn(X, y, w, b, alpha)\n",
    "        costs.append(cost)\n",
    "        if i % 100 == 0:\n",
    "            print(f'cost at iteration {i}: {cost}, w: {w}, b: {b}')\n",
    "    end = time.time()\n",
    "    print(f'execution time: {end - start}')\n",
    "    return w, b, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent multiple variables, vectorized implementation\n",
      "cost at iteration 0: 49518.0, w: [2.41334667e-01 5.58666667e-04 1.83666667e-04 6.03500000e-03], b: 0.000145\n",
      "cost at iteration 100: 696.0010595124638, w: [ 0.20235171  0.00079796 -0.00099658 -0.00219736], b: -0.00011985961877688935\n",
      "cost at iteration 200: 694.9313476914755, w: [ 0.20253446  0.00112715 -0.00214349 -0.00940619], b: -0.0003596578183953631\n",
      "cost at iteration 300: 693.8709864577188, w: [ 0.2027164   0.00145611 -0.00328876 -0.01658286], b: -0.0005983240279392168\n",
      "cost at iteration 400: 692.819893023782, w: [ 0.20289753  0.00178484 -0.00443238 -0.02372751], b: -0.000835863270686938\n",
      "cost at iteration 500: 691.7779853352556, w: [ 0.20307785  0.00211335 -0.00557437 -0.03084027], b: -0.0010722805476294614\n",
      "cost at iteration 600: 690.7451820642364, w: [ 0.20325736  0.00244162 -0.00671473 -0.0379213 ], b: -0.001307580837569055\n",
      "cost at iteration 700: 689.7214026029071, w: [ 0.20343608  0.00276967 -0.00785347 -0.04497072], b: -0.0015417690972177698\n",
      "cost at iteration 800: 688.7065670571469, w: [ 0.20361399  0.00309749 -0.00899059 -0.05198869], b: -0.0017748502612954461\n",
      "cost at iteration 900: 687.7005962402227, w: [ 0.20379112  0.00342509 -0.01012611 -0.05897533], b: -0.0020068292426272975\n",
      "execution time: 0.06378006935119629\n",
      "-----------------------------------\n",
      "w: [ 0.20396569  0.00374919 -0.0112487  -0.0658614 ]\n",
      "b: -0.002235407530932535\n",
      "prediction: [426.18530497 286.16747201 171.46763087]\n",
      "original: [460 232 178]\n"
     ]
    }
   ],
   "source": [
    "w_initial = np.zeros_like(w_init)\n",
    "b_initial = 0\n",
    "alpha = 5.0e-7\n",
    "iterations = 1000\n",
    "\n",
    "w, b, costs = compute_gradient_iter(X_train, y_train, w_initial, b_initial, alpha, iterations, compute_gradient)\n",
    "print('-----------------------------------')\n",
    "print(f'w: {w}')\n",
    "print(f'b: {b}')\n",
    "y_hat = predict(X_train, w, b)\n",
    "print(f'prediction: {y_hat}')\n",
    "print(f'original: {y_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent multiple variables, vectorized implementation\n",
      "cost at iteration 0: 49518.0, w: [2.41334667e-01 5.58666667e-04 1.83666667e-04 6.03500000e-03], b: 0.000145\n",
      "cost at iteration 100: 696.0010595124644, w: [ 0.20235171  0.00079796 -0.00099658 -0.00219736], b: -0.00011985961877688931\n",
      "cost at iteration 200: 694.9313476914762, w: [ 0.20253446  0.00112715 -0.00214349 -0.00940619], b: -0.00035965781839536286\n",
      "cost at iteration 300: 693.8709864577195, w: [ 0.2027164   0.00145611 -0.00328876 -0.01658286], b: -0.0005983240279392168\n",
      "cost at iteration 400: 692.8198930237817, w: [ 0.20289753  0.00178484 -0.00443238 -0.02372751], b: -0.0008358632706869382\n",
      "cost at iteration 500: 691.7779853352548, w: [ 0.20307785  0.00211335 -0.00557437 -0.03084027], b: -0.0010722805476294612\n",
      "cost at iteration 600: 690.7451820642369, w: [ 0.20325736  0.00244162 -0.00671473 -0.0379213 ], b: -0.0013075808375690545\n",
      "cost at iteration 700: 689.7214026029069, w: [ 0.20343608  0.00276967 -0.00785347 -0.04497072], b: -0.0015417690972177696\n",
      "cost at iteration 800: 688.706567057147, w: [ 0.20361399  0.00309749 -0.00899059 -0.05198869], b: -0.001774850261295446\n",
      "cost at iteration 900: 687.7005962402227, w: [ 0.20379112  0.00342509 -0.01012611 -0.05897533], b: -0.0020068292426272975\n",
      "execution time: 0.04274749755859375\n",
      "-----------------------------------\n",
      "w: [ 0.20396569  0.00374919 -0.0112487  -0.0658614 ]\n",
      "b: -0.002235407530932535\n",
      "prediction: [426.18530497 286.16747201 171.46763087]\n",
      "original: [460 232 178]\n"
     ]
    }
   ],
   "source": [
    "w_initial = np.zeros_like(w_init)\n",
    "b_initial = 0\n",
    "alpha = 5.0e-7\n",
    "iterations = 1000\n",
    "\n",
    "w, b, costs = compute_gradient_iter(X_train, y_train, w_initial, b_initial, alpha, iterations, compute_gradient_without_vectorization)\n",
    "print('-----------------------------------')\n",
    "print(f'w: {w}')\n",
    "print(f'b: {b}')\n",
    "y_hat = predict(X_train, w, b)\n",
    "print(f'prediction: {y_hat}')\n",
    "print(f'original: {y_train}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature and parameter values\n",
    "\n",
    "* when the range is large $300 - 2000 $ then $w$ will be small\n",
    "* when the range is small $1 - 5 $ then $w$ will be large\n",
    "\n",
    "#### feature scaling\n",
    "\n",
    "* make sure features are on a similar scale\n",
    "* divide the input values by the range $(max - min)$ or by $max$\n",
    "\n",
    "\n",
    "\n",
    "#### Mean normalization\n",
    "\n",
    "$$ x_i = \\dfrac{x_i - \\mu_i}{s_i} $$\n",
    "\n",
    "$$ \\mu_i = \\dfrac{1}{m} \\sum_{j=1}^{m} x_i^{(j)} $$\n",
    "$$ s_i = max(x_i) - min(x_i) $$\n",
    "\n",
    "where:\n",
    "* $x_i$ is the $i^{th}$ feature\n",
    "* $\\mu_i$ is the average of the $i^{th}$ feature\n",
    "* $s_i$ is the range of the $i^{th}$ feature\n",
    "\n",
    "#### z-score normalization:\n",
    "\n",
    "$$ x_i = \\dfrac{x_i - \\mu_i}{\\sigma_i} $$\n",
    "$$ \\sigma_i = \\sqrt{\\dfrac{1}{m} \\sum_{j=1}^{m} (x_i^{(j)} - \\mu_i)^2} $$\n",
    "\n",
    "where:\n",
    "* $x_i$ is the $i^{th}$ feature\n",
    "* $\\mu_i$ is the average of the $i^{th}$ feature\n",
    "* $\\sigma_i$ is the standard deviation of the $i^{th}$ feature\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
